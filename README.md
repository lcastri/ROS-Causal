# Experimental Evaluation of ROS-Causal in Real-World Human-Robot Spatial Interaction Scenarios

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10844902.svg)](https://doi.org/10.5281/zenodo.10844902)

<p float="left">
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A1.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A2.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A3.gif" width="32%" height="32%" />
    <br>
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A4.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A5.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A6.gif" width="32%" height="32%" />
    <!-- <br>
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A7.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A8.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A9.gif" width="32%" height="32%" />
    <br>
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A10.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A11.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A12.gif" width="32%" height="32%" />
    <br>
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A13.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A14.gif" width="32%" height="32%" />
    <img src="https://github.com/lcastri/ROS-Causal/blob/master/images/A15.gif" width="32%" height="32%" /> -->
</p>

This repository contains the scripts for processing and visualising our dataset named "Human-Robot Spatial Interaction Dataset for Causal Analysis from Mobile Platforms".

The dataset captures a Human-Robot Spatial Interaction (HRSI) scenario between a person and the TIAGo robot. It includes: 
* rosbags containing: Velodyne LiDAR point cloud, robot and human state (position, orientation and velocities);
* CSV files containing trajectories of the person and the robot generated by post-processing the rosbags;
* the map of the environment extracted from the TIAGo robot.

**15 participants** took part in the experiment, with the dataset capturing **5 minutes of Human-Robot Spatial Interaction (HRSI) motion for each participant**.

The dataset is available on Zenodo at this [link](https://zenodo.org/records/10844902)

## RosBag Processing

## Trajectory Plot and Animation 
